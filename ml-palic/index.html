<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8"/>

    <title>Machine learning and content management</title>

    <meta name="author" content="Valery 'valthebald' Lourie"/>

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>

    <link rel="stylesheet" href="../css/reveal.min.css">
    <link rel="stylesheet" href="../css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="../lib/css/zenburn.css">
    <link rel="stylesheet" href="../css/highlight-default.min.css"/>

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
        document.write('<link rel="stylesheet" href="../css/print/' + ( window.location.search.match(/print-pdf/gi) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">');
    </script>

    <!--[if lt IE 9]>
    <script src="../lib/js/html5shiv.js"></script>
    <![endif]-->
    <script src="../js/highlight.min.js"></script>
</head>

<body>

<div class="reveal">

    <!-- Any section element inside of this container is displayed as a slide -->
    <section class="slides">
        <!-- Session title -->
        <section class="title">
            <h1>Machine learning and content management</h1>
            <h2><a href="//bit.ly/ml-palic">bit.ly/ml-palic</a></h2>

            <aside class="notes"><pre>
				Slides are hosted on github, press 's' if you want to read speaker notes too.
				All links mentioned in the presentation will be posted in the last slide
                This talk is largely based on my Drupal Europe presentation
                Main feedback that I got is that the session contained too much of math.
                I cannot completely avoid mathematics when talking about machine learning,
                but tried to reduce its usage to reasonable minimum. Hold tight!
			</pre></aside>
        </section>
        <!-- Speakers -->
        <section class="speakers">
            <h2>About me</h2>
            <div class="speaker">
                <img src="media/valthebald.jpg" class="speaker">
                <h2>Valery "valthebald" Lourie</h2>
                <div class="role">Drupal Developer, FFW Agency</div>
                <div class="twitter">valthebald</div>
            </div>
        </section>


        <!-- Regular list slide -->
        <section class="slide">
            <h2>Agenda</h2>
            <h3>What we <em>will</em> discuss</h3>
            <ul>
                <li class="fragment">What is machine learning</li>
                <li class="fragment">Supervised learning</li>
                <li class="fragment">Linear regression</li>
                <li class="fragment">Online learning</li>
                <li class="fragment">References</li>
            </ul>
            <aside class="notes"><pre>
        This short introduction is inspired by the excellent coure of Stanford University,
        available online (link will be in the references section).
    </pre></aside>
        </section>
        <!-- Regular slide -->

        <section class="slide">
            <h2>Agenda</h2>

            <h3>What we will <em>not</em> discuss</h3>

            <ul>
                <li>Use of commercial APIs</li>
                <li>Auto tagging</li>
                <li>Neural networks</li>
                <li>Non-supervised learning</li>
                <li class="fragment">When robots will take over the world</li>
            </ul>
            <aside class="notes"><pre>
Since machine learning is a very wide topic, we will be able to discuss only small part of it.
Luckily, there are many resources, including Drupal-specific, that can help you in using ML in your everyday activity
So first, we will skip ready, commercial APIs from companies like Google (Cloud API), or MicroSoft (Azure Cognitive Services).
Both have Drupal modules and work with either pretrained or DIY data sets, for the real life examples of integrating Google SDKs with local
news service Patch.com you can watch Pantheon's webinar (link in the end).

We also will not talk about tasks that have known solutions, and even Drupal modules - for the list of those I recommend reading OpenSense post
on top Drupal modules that use AI.

In other words, let's jump to the land of unknown, and start from the basics - what is machine learning
    </pre></aside>
        </section>


        <section class="quote">
            <blockquote>Field of study that gives computers the ability to learn without being explicitly programmed</blockquote>
            <small class="author">Arthur Samuel</small>
            <aside class="notes"><pre>
						Here's one of the first definitions of machine learning.
						Being intuitive, it doesn't give us much idea of where to start from
						</pre>
            </aside>
        </section>


        <section class="quote">
            <blockquote style="font-size:100%">A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</blockquote>
            <small class="author">Tom Mitchell</small>
            <aside class="notes"><pre>
						This one is more "formal". Let's now understand what are variable E, T, and P
						</pre>
        </section>

        <section class="slide">
            <img src="media/umami.png" class="no-frame" alt="Umami" />

            <aside class="notes"><pre>
						Let's imagine you build a site with multiple pages. You can have different monetizing models: you can sell items,
						profit from advertiser, or get financed by some government organisation based on your traffic numbers.
						You have a lot of items to "sell" to the customer (in quotes because money is not always involved in the process),
						and you constantly adding new items.

						You have few areas where you list important (from your PoV) items: top menu, featured block etc.
						You can even measure performance of those items using A/B testing tools.

						But if you a new item with no known performance, how do you decide whether to replace existing featured items? Let's try
						to formulate the question in the terms of the previous slide.

						T - predicting CTR, average purchase, time on page etc.
						E - data collected from analytics data or/and webserver log analysis
						P - difference between predicted and actual performance.
					</pre>
            </aside>
        </section>


        <section class="slide">
            <h2>Supervised learning</h2>
            <h3>We know "correct" values</h3>

            <ul>
                <li>Regression</li>
                <li>Classification</li>
            </ul>

            <aside class="notes"><pre>
			Before digging into Umami dilemma, let's first talk about machine learning tasks types - i.e. T in the definition of machine learning
			One group of task classes is supervised learning. It's when our experimental data E is "labelled",
			i.e. for every item in E we know "correct" value - i.e. what is CTR.
            Regression - predicted values are within continious range, i.e.
            Given a picture of Male/Female, We have to predict his/her age on the basis of given picture.
            Classification - predicted value is boolean, i.e. given a picture of Male/Female, We have to predict Whether He/She is of High school, College, Graduate age. Another Example for Classification - Banks have to decide whether or not to give a loan to someone on the basis of his credit history.
        </pre></aside>
        </section>

        <section class="slide">
            <h2>Unsupervised learning</h2>
            <h3>Unlabeled data</h3>

            <ul>
                <li>Clustering</li>
                <li>Dimension reduction</li>
                <li>Anomaly detection</li>
            </ul>

            <aside class="notes"><pre>
            Clustering - group unlabeled data into clusters of values
			Dimension reduction -

        </pre></aside>
        </section>

        <section class="section">
            <h2>Learning flow</h2>
            <ul>
                <li>Initialize hypothesis function parameters</li>
                <li>Learn using training set</li>
                <li>Validate using validation set</li>
            </ul>

            <aside class="notes">Here is the training flow, independent of class type
                Learning is essentially minimizing the chosen cost function.
                What do depend on the class type, are hypothesis and cost functions</aside>
        </section>

        <section class="slide">
            <h2>E - experimental data</h2>

            <h3>Features</h3>
            <ul>
                <li>Position in the list</li>
                <li>Visitor demographics (country/language)</li>
                <li>Time (weekday/hour)</li>
                <li>Previous history of actions</li>
                <li>Current page data (taxonomy)</li>
                <li>Update date of the content</li>
            </ul>

            <h3>Labels</h3>

            <ul>
                <li>Time on page</li>
                <li>CTR</li>
                <li>Probability of purchase</li>
                <li>Average purchase amount</li>
            </ul>

            <aside class="notes"><pre>
						Given that all of values that we need to predict lay in continuous range, we're talking about regression class of tasks.
						Most popular hypothesis function used in regression task is linear function.
					</pre>
            </aside>
        </section>

        <section class="slide">
            <h2>Linear regression</h2>
            <h3>Experimental data</h3>

            <img class="no-frame" src="media/ctr-position.png" alt="CTR dependent on position" />

            <div>Taken from https://www.wordstream.com/adwords-click-through-rate</div>

            <aside class="notes">
                Let's start with regression task that has one label and one feature. Here we can see experimental data on CTR as
                a function of position in Google's search results. I assume similar date you can obtain by measuring CTR of products
                depending on their position in the featured products block.
            </aside>

        </section>

        <section class="slide">
            <h2>Linear regression</h2>
            <h3>Hypothesis</h3>

            \[ h = \mathbf{&theta;}_0 + \mathbf{&theta;}_1 \times X\]

            <img class="no-frame" src="media/linear-position.png" alt="CTR dependent on position" />

            <ul>
                <li class="fragment">\[Light green:  \mathbf{&theta;}_0 = 0; \mathbf{&theta;}_1 = 0.1; \]</li>
                <li class="fragment">\[Light blue:  \mathbf{&theta;}_0 = -0.3; \mathbf{&theta;}_1 = 0.1; \]</li>
                <li class="fragment">\[Dark blue:  \mathbf{&theta;}_0 = 9; \mathbf{&theta;}_1 = -0.05; \]</li>
            </ul>

            <aside class="notes">
                The simplest and by far the most popular method for approximating experimental data is linear regression,
                i.e. hypothesis function - our prediction function - is the linear function
                Mathematicians use these fancy variable names theta to confuse everyone, but essentially function on the slide describes
                a line. Line has 2 parameters: offset from 0 (theta_0) and slope (theta_1)

                Our task is to find such values of theta that "fit better" experimental data.

                Intuitively it's clear that dark blue line more accurately describes the experimental data
                (thus can predict CTR for the new products).

                More accurate means smaller difference between predicted and experimental data.

                What we need to do is go over all experimental data, and sum the distance from predicted and actial values
            </aside>

        </section>

        <section class="section">
            <h2>Linear regression</h2>

            <h3>Hypothesis</h3>

            \[ h = \mathbf{&theta;}_0 + \mathbf{&theta;}_1 \times X\]

            <h3>Cost function</h3>

            \[ J(&theta;) = \frac1{2m} \sum_{i=1}^m \left( h(X^{(i)}) - y^{(i)} \right)^2 \]

            <aside class="notes">
                Don't be scared, this is the same formula - sum of distances between predicted and experimental data,
                divided by the number of samples.
                m - number of samples in the training set.
            </aside>
        </section>

        <section class="section">
            <h2>Gradient descent</h2>

            <img class="no-frame" src="media/descent.png" /><br/>
            <aside class="notes">
                To find the best values of theta, we start from random values (typically from zeros, but can be any starting point),
                perform small change of theta, and then change them towards  the direction of smaller error values

                In general case, final result of gradient descent is not necessarily the best one - pitfall of local minimums.
                Luckily, for linear regression cost function is always convex, so we end up at the global minimum.
                (Another reason for popularity of linear regression)
            </aside>
        </section>

        <section class="section">
            <h2>Gradient descent</h2>

            <img class="no-frame" src="media/descent-linear.png" />

            <div>By Indeed123 - commons, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=5508870</div>
            <aside class="notes">
                Convex function of 2 variables with one global maximum
            </aside>
        </section>

        <section class="slide">
            <h2>Gradient descent</h2>

            <h3>Formal definition</h3>

            \[ \mathbf{&theta;}_j = \mathbf{&theta;}_j - &alpha; \frac{\partial}{\partial\mathbf{&theta;}_j} J(&theta;) \]

            <div class="fragment">Repeat until convergence</div>

            <aside class="notes">
                m - number of samples in the training set.
            </aside>
        </section>

        <section class="slide">
            <h2>Gradient descent with one feature</h2>
            <h3>Derivatives</h3>

            <div class="fragment">
                \[ \frac{\partial}{\partial\mathbf{&theta;}_0} J(&theta;) = \frac1m \sum_{i=1}^m \left( h(X^{(i)}) - y^{(i)} \right) \]
                \[ \frac{\partial}{\partial\mathbf{&theta;}_1} J(&theta;) = \frac1m \sum_{i=1}^m \left( h(X^{(i)}) - y^{(i)} \right) \times X^{(i)}\]
            </div>
        </section>

        <section class="slide">
            <h2>Linear regression</h2>
            <h3>Experimental data</h3>

            <img class="no-frame" src="media/ctr-position.png" alt="CTR dependent on position" />

            <div>You can't bring the cost to 0!</div>

            <aside class="notes">
                You can see that there are points that have the same feature value, but different label.
                That means one dimension, one feature linear function is not a good choice.
            </aside>

        </section>

        <section class="slide">
            <h2>More features</h2>

            \[h = \mathbf{&theta;}_0 + \mathbf{&theta;}_1 \times {X}_1 \]

            <div class="fragment"> \[h = \mathbf{&theta;}_0 + \mathbf{&theta;}_1 \times {X}_1 + \mathbf{&theta;}_2 \times {X}_2\] </div>
            <div class="fragment"> \[h = \mathbf{&theta;}_0 + \mathbf{&theta;}_1 \times {X}_1 + ... \mathbf{&theta;}_N \times {X}_N\] </div>
            <div class="fragment"> Let's agree that \[ {X}_0 = 0 \]</div>
            <div class="fragment"> \[h = \mathbf{&theta;}_0 \times {X}_0 + \mathbf{&theta;}_1 \times {X}_1 +  \mathbf{&theta;}_2 \times {X}_2 + ... \mathbf{&theta;}_N \times {X}_N\] </div>
            <div class="fragment"> \[h = \mathbf{&theta;} \times {X}\] </div>

            <aside class="notes">
                Adding new features is also a way to have non-linear derivatives of original features, but still use linear regression.
                In our example my guess would be that logarithm function is better approximation than line.
                But we still can use linear regression, if we add a new feature theta_2, that will be equal to logarithm of theta_1

                There are also naturally other features, like price, country, brand, date of release (freshness) etc.
                Country and brand are themselves not single, but multiple features:
                If you're not Donald Trump, you're probably won't tell that US is no.1, UK is no.2 etc. - but even then, how can you assign values
                to other countries? Correct way is to have multiple features, each one having value 0 or 1 - so if we sell to 50 countries,
                we should define 50 binary features.
            </aside>
        </section>

        <section class="slide">
            <h2>Gradient descent multiple features</h2>

            <div class="fragment">
                \[ \frac{\partial}{\partial\mathbf{&theta;}_0} J(&theta;) = \frac1m \sum_{i=1}^m \left( h(X^{(i)}) - y^{(i)} \right) \]
                \[ \frac{\partial}{\partial\mathbf{&theta;}_1} J(&theta;) = \frac1m \sum_{i=1}^m \left( h(X^{(i)}) - y^{(i)} \right) \times \mathbf{X}_1^{(i)}\]
                \[ \frac{\partial}{\partial\mathbf{&theta;}_2} J(&theta;) = \frac1m \sum_{i=1}^m \left( h(X^{(i)}) - y^{(i)} \right) \times \mathbf{X}_2^{(i)}\]
                ...
                \[ \frac{\partial}{\partial\mathbf{&theta;}_N} J(&theta;) = \frac1m \sum_{i=1}^m \left( h(X^{(i)}) - y^{(i)} \right) \times \mathbf{X}_N^{(i)}\]
            </div>
        </section>

        <section>
            <h2>Less features!</h2>

            <img class="no-frame" src="media/linear_regular.png" />

            <aside class="notes">
                Let's penalize new features!
                Better way to use/select non-linear features is to use layered structure (neural networks),
                that can itself learn importance of certain feature combinations.
            </aside>
        </section>

        <section>

            <h2>Validating the model</h2>

            <ul>
                <li>Training set: 60%</li>
                <li>Cross validation set: 20%</li>
                <li>Test set: 20%</li>
            </ul>

            <aside class="notes">

            <ol>
                <li>Optimize the parameters in Î˜ using the training set for each polynomial degree.</li>
                <li>Find polynomial degree with least error using cross validation</li>
                <li>Estimate generalization error using test set</li>
            </ol>
            </aside>

        </section>

        <section>
            <h2>Working with large data set</h2>

            <h3>Stochastic gradient descent</h3>

            <img src="media/ctr-position.png" alt="Experiment" />

            <div>\[ &theta; = &theta; - &alpha; \times \frac1{2m} \sum_{i=1}^m \left( h(X^{(i)}) - y^{(i)} \right) \times X \]</div>

            <aside class="notes"><pre>
                Let's address the first challenge when working with website visitors data - it is huge.
                We calculate the cost function by going over ALL data items, and compare predicted and actual label values.

                We do that for every step of changing parameters. This method is reliable, but slow
            </pre></aside>
        </section>

        <section>
            <h2>Working with large data set</h2>

            <h3>Stochastic gradient descent</h3>

            <img src="media/ctr-position.png" alt="Experiment" />

            <ul>
                <li>Shuffle training set</li>
                <li>Perform gradient descent for single example</li>
                <li>Choose low &alpha;!</li>
            </ul>
            <aside class="notes"><pre>
                What if we take even smaller steps, but get some benefit from going over not the full set of experimental data,
                but only one?

                This method called stochastic gradient descent. Due to its nature, it's not guaranteed that you only go down finding the global minimum of cost function,
                but if you have large, randomly distributed set, you'll get to the point - and much quicker than in case of "regular" gradient descent
                (which is also called batch gradient descent)
            </pre></aside>
        </section>

        <section>
            <h2>Online learning</h2>

            <ul>
                <li>Process training examples as they go</li>
                <li>Adjust parameters &theta; after every example</li>
                <li>Throw examples immediately</li>
                <li class="fragment">Bonus: adjust to changing users preferences</li>
            </ul>
            <aside class="notes"><pre>
                Online learning is the process when you don't have static experimental data,
                but instead you constantly get a stream of new items.
                This is very similar to stochastic gradient descent process that we have seen on the previous slide.
                We get data items one by one, adjust theta values by small portion, and immediately throw away incoming data.
                Bonus - we adjust to changing user preferences!
            </pre></aside>
        </section>

        <section class="slide">
            <h2>Features</h2>
            <ul>
                <li>Visitor demographics (country/language)</li>
                <li>Time (weekday/hour)</li>
                <li>Previous history of actions</li>
                <li>Current page data (taxonomy)</li>
                <li>Update date of the content</li>
            </ul>

            <h2>Split test and feedback</h2>
        </section>

        <section class="slide">
            <h2>Q & A</h2>
            <ul>
                <li><a href="https://www.drupaleurope.org/session/content-management-ai-driven-world">Drupal Europe session (with video recording)</a></li>
                <li><a href="https://www.coursera.org/learn/machine-learning">Machine learning on Coursera</a></li>
                <li><a href="https://www.quora.com/What-is-the-difference-between-supervised-and-unsupervised-learning-algorithms">Quora on difference supervised vs non-supervised</a> </li>
                <li><a href="https://www.gnu.org/software/octave/">Octave</a></li>
                <li><a href="https://github.com/php-ai/php-ml">PHP-ML</a></li>
                <li><a href="https://pantheon.io/resources/artificial-intelligence-your-cms">Artificial intelligence and your CMS</a></li>
                <li><a href="https://opensenselabs.com/blog/tech/top-2018-drupal-modules-using-artificial-intelligence">Top Drupal modules using AI</a> </li>
            </ul>
        </section>


    </section>
</div>

<script src="../lib/js/head.min.js"></script>
<script src="../js/reveal.min.js"></script>

<script>

    // Full list of configuration options available here:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

        // Optional libraries used to extend on reveal.js
        dependencies: [
            { src: '../lib/js/classList.js', condition: function () {
                    return !document.body.classList;
                } },
            { src: '../plugin/markdown/marked.js', condition: function () {
                    return !!document.querySelector('[data-markdown]');
                } },
            { src: '../plugin/markdown/markdown.js', condition: function () {
                    return !!document.querySelector('[data-markdown]');
                } },
            { src: '../plugin/highlight/highlight.js', async: true, callback: function () {
                    hljs.initHighlightingOnLoad();
                } },
            { src: '../plugin/math/math.js', async: true, condition: function () {
                    return !!document.body.classList;
                } },
            { src: '../plugin/zoom-js/zoom.js', async: true, condition: function () {
                    return !!document.body.classList;
                } },
            { src: '../plugin/notes/notes.js', async: true, condition: function () {
                    return !!document.body.classList;
                } }
        ]
    });

    Reveal.addEventListener('fragmentshown');
</script>
</body>
</html>
